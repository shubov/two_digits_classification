{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KoYQeQD9sxW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applied AI Lab WS2022 Introductory Task\n",
    "\n",
    "Please solve the following task as part of your application to the Applied AI Lab.\n",
    "Since the lab depends on the participants being able to implement (deep) machine learning solutions, solving this simple introductory task is a requirement for acceptance.\n",
    "However, it is not important to maximize the performance or do excessive optimization.\n",
    "\n",
    "You are given a dataset (in Studip) of handwritten numbers with two digits.\n",
    "Your task is to construct and train a classifier to predict the number from the image input.\n",
    "Specifically, please:\n",
    "1. Train an sklearn classifier for this task.\n",
    "2. Train a deep learning model for this task with testing accuracy over 80% (there is no hidden test set, so do not overfit on the given test data).\n",
    "3. Send a link to your solution (a single Google Colab notebook) as part of your application mail. A copy of this notebook is available at https://colab.research.google.com/drive/1MrL2dlC5c29sRcZDw3P8rk4KmyKpWd0U?usp=sharing.\n",
    "\n",
    "Additional information:\n",
    "* The dataset can be easily loaded with `torchvision.datasets.ImageFolder`.\n",
    "* You can use any deep learning framework you are comfortable with.\n",
    "* We recommend working on Google Colab. To have access to the dataset, see https://colab.research.google.com/notebooks/io.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "_OiMK0jL9rs8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "img_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size,img_size)),\n",
    "    #transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "train_data_path = \"./images/train\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "test_data_path = \"./images/test\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms, is_valid_file=check_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "batch_size=100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, img_size*img_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        res=torch.Tensor()\n",
    "        for i in range(len(x)):\n",
    "            try:\n",
    "                res = torch.cat((res, torch.argmax(x[i])), dim=1)\n",
    "            except:\n",
    "                res = torch.argmax(x[i])\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(simplenet.parameters(), lr=0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "SimpleNet(\n  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=100, bias=True)\n)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "simplenet.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        valid_loss = 0.0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets)\n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/xkd5blwd6hz99wjjbm45kwsw0000gn/T/ipykernel_58320/967564360.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.43, Validation Loss: 1.26, accuracy = 0.64\n",
      "Epoch: 1, Training Loss: 1.09, Validation Loss: 0.91, accuracy = 0.74\n",
      "Epoch: 2, Training Loss: 0.75, Validation Loss: 0.86, accuracy = 0.75\n",
      "Epoch: 3, Training Loss: 0.55, Validation Loss: 0.82, accuracy = 0.77\n",
      "Epoch: 4, Training Loss: 0.42, Validation Loss: 0.78, accuracy = 0.79\n",
      "Epoch: 5, Training Loss: 0.33, Validation Loss: 0.80, accuracy = 0.78\n",
      "Epoch: 6, Training Loss: 0.24, Validation Loss: 0.77, accuracy = 0.81\n",
      "Epoch: 7, Training Loss: 0.19, Validation Loss: 0.80, accuracy = 0.80\n",
      "Epoch: 8, Training Loss: 0.14, Validation Loss: 0.92, accuracy = 0.81\n"
     ]
    }
   ],
   "source": [
    "train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,test_data_loader, epochs=9, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "ailab22_intro_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}